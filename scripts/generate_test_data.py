#!/usr/bin/env python3
"""
–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤.
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–µ–Ω—å—à–µ —Ç–µ—Å—Ç–æ–≤, –Ω–æ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å—é.
"""

import os
import json
import random
from datetime import datetime
from typing import List, Dict, Any
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.generators.dass_generator import DassGenerator
from src.generators.validator import DassValidator


class OptimizedTestDataGenerator:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.alphabet = [chr(ord('A') + i) for i in range(26)]  # A-Z
    
    def generate_optimized_test_suite(self) -> Dict[str, List[Dict]]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤
        
        –í—Å–µ–≥–æ: 30 —Ç–µ—Å—Ç–æ–≤ (–≤–º–µ—Å—Ç–æ 110), –Ω–æ –≤—Å–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω—ã
        """
        print("üöÄ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        print("=" * 70)
        
        test_suite = {
            "tiny": self._generate_tiny_tests(5),       # 5 —Ç–µ—Å—Ç–æ–≤: 2-3 —ç–ª–µ–º–µ–Ω—Ç–∞
            "small": self._generate_small_tests(7),     # 7 —Ç–µ—Å—Ç–æ–≤: 4-6 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            "medium": self._generate_medium_tests(6),   # 6 —Ç–µ—Å—Ç–æ–≤: 7-10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            "large": self._generate_large_tests(4),     # 4 —Ç–µ—Å—Ç–∞: 11-15 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            "xlarge": self._generate_xlarge_tests(3),   # 3 —Ç–µ—Å—Ç–∞: 16-20 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            "stress": self._generate_stress_tests(2),   # 2 —Ç–µ—Å—Ç–∞: 21-26 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        }
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
        special_tests = self._generate_special_cases()
        test_suite["special"] = special_tests
        
        return test_suite
    
    def _generate_tiny_tests(self, count: int) -> List[Dict]:
        """–ú–∞–ª–µ–Ω—å–∫–∏–µ —Ç–µ—Å—Ç—ã (2-3 —ç–ª–µ–º–µ–Ω—Ç–∞)"""
        tests = []
        for i in range(1, count + 1):
            n_elements = random.choice([2, 3])
            n_sources = random.choice([2, 3])
            
            test = self._generate_validated_test(
                n_elements=n_elements,
                n_sources=n_sources,
                group="tiny",
                test_id=f"tiny_{i:03d}"
            )
            tests.append(test)
        
        print(f"  ‚úì tiny: {count} —Ç–µ—Å—Ç–æ–≤ (2-3 —ç–ª–µ–º–µ–Ω—Ç–∞)")
        return tests
    
    def _generate_small_tests(self, count: int) -> List[Dict]:
        """–ú–∞–ª–µ–Ω—å–∫–∏–µ —Ç–µ—Å—Ç—ã (4-6 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)"""
        tests = []
        for i in range(1, count + 1):
            n_elements = random.choice([4, 5, 6])
            n_sources = random.choice([2, 3, 4])
            
            test = self._generate_validated_test(
                n_elements=n_elements,
                n_sources=n_sources,
                group="small",
                test_id=f"small_{i:03d}"
            )
            tests.append(test)
        
        print(f"  ‚úì small: {count} —Ç–µ—Å—Ç–æ–≤ (4-6 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
        return tests
    
    def _generate_medium_tests(self, count: int) -> List[Dict]:
        """–°—Ä–µ–¥–Ω–∏–µ —Ç–µ—Å—Ç—ã (7-10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)"""
        tests = []
        for i in range(1, count + 1):
            n_elements = random.randint(7, 10)
            n_sources = random.choice([2, 3, 4])
            
            test = self._generate_validated_test(
                n_elements=n_elements,
                n_sources=n_sources,
                group="medium",
                test_id=f"medium_{i:03d}"
            )
            tests.append(test)
        
        print(f"  ‚úì medium: {count} —Ç–µ—Å—Ç–æ–≤ (7-10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
        return tests
    
    def _generate_large_tests(self, count: int) -> List[Dict]:
        """–ë–æ–ª—å—à–∏–µ —Ç–µ—Å—Ç—ã (11-15 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)"""
        tests = []
        for i in range(1, count + 1):
            n_elements = random.randint(11, 15)
            n_sources = random.choice([2, 3])
            
            test = self._generate_validated_test(
                n_elements=n_elements,
                n_sources=n_sources,
                group="large",
                test_id=f"large_{i:03d}"
            )
            tests.append(test)
        
        print(f"  ‚úì large: {count} —Ç–µ—Å—Ç–æ–≤ (11-15 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
        return tests
    
    def _generate_xlarge_tests(self, count: int) -> List[Dict]:
        """–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ —Ç–µ—Å—Ç—ã (16-20 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)"""
        tests = []
        for i in range(1, count + 1):
            n_elements = random.randint(16, 20)
            n_sources = 2  # –¢–æ–ª—å–∫–æ 2 –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            
            test = self._generate_validated_test(
                n_elements=n_elements,
                n_sources=n_sources,
                group="xlarge",
                test_id=f"xlarge_{i:03d}"
            )
            tests.append(test)
        
        print(f"  ‚úì xlarge: {count} —Ç–µ—Å—Ç–æ–≤ (16-20 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
        return tests
    
    def _generate_stress_tests(self, count: int) -> List[Dict]:
        """–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã (21-26 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)"""
        tests = []
        for i in range(1, count + 1):
            n_elements = random.randint(21, 26)
            n_sources = 2  # –¢–æ–ª—å–∫–æ 2 –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            
            test = self._generate_validated_test(
                n_elements=n_elements,
                n_sources=n_sources,
                group="stress",
                test_id=f"stress_{i:03d}"
            )
            tests.append(test)
        
        print(f"  ‚úì stress: {count} —Ç–µ—Å—Ç–æ–≤ (21-26 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
        return tests
    
    def _generate_validated_test(self, n_elements: int, n_sources: int, 
                               group: str, test_id: str, max_attempts: int = 5) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ç–µ—Å—Ç.
        –ü–æ–≤—Ç–æ—Ä—è–µ—Ç –ø–æ–ø—ã—Ç–∫–∏ –¥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–ª–∏–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞.
        """
        attempts = 0
        while attempts < max_attempts:
            attempts += 1
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã
            elements = self.alphabet[:n_elements]
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç
            test_data = DassGenerator.generate_simple(
                elements=elements,
                n_sources=n_sources,
                density=self._get_optimal_density(n_elements),
                include_empty=random.choice([True, False])
            )
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º
            is_valid, errors = DassValidator.validate_data(test_data)
            
            if is_valid:
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                test_data["metadata"].update({
                    "description": f"{group.capitalize()} —Ç–µ—Å—Ç: {n_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, {n_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
                    "test_group": group,
                    "test_id": test_id,
                    "validation_attempts": attempts
                })
                return test_data
            else:
                # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞, –≤—ã–≤–æ–¥–∏–º –æ—à–∏–±–∫—É
                if attempts == max_attempts:
                    print(f"    ‚ö†Ô∏è  –¢–µ—Å—Ç {test_id}: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π —Ç–µ—Å—Ç –ø–æ—Å–ª–µ {max_attempts} –ø–æ–ø—ã—Ç–æ–∫")
                    print(f"       –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {errors[0] if errors else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}")
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–µ–π—à–∏–π –≤–∞–ª–∏–¥–Ω—ã–π —Ç–µ—Å—Ç
        return self._create_minimal_valid_test(n_elements, n_sources, group, test_id)
    
    def _get_optimal_density(self, n_elements: int) -> float:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Ñ—Ä–µ–π–º–∞"""
        if n_elements <= 3:
            return 0.4
        elif n_elements <= 6:
            return 0.3
        elif n_elements <= 10:
            return 0.2
        elif n_elements <= 15:
            return 0.1
        elif n_elements <= 20:
            return 0.05
        else:
            return 0.03
    
    def _create_minimal_valid_test(self, n_elements: int, n_sources: int,
                                 group: str, test_id: str) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–∞–ª–∏–¥–Ω—ã–π —Ç–µ—Å—Ç"""
        elements = self.alphabet[:n_elements]
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–∞–ª–∏–¥–Ω—ã–π BPA: –≤—Å–µ –º–∞—Å—Å–∞ –Ω–∞ –æ–¥–Ω–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
        minimal_bba = {f"{{{elements[0]}}}": 1.0}
        
        test_data = {
            "metadata": {
                "format": "DASS",
                "version": "1.0",
                "description": f"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π {group} —Ç–µ—Å—Ç: {n_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, {n_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
                "test_group": group,
                "test_id": test_id,
                "is_minimal_fallback": True,
                "generated_at": datetime.now().isoformat()
            },
            "frame_of_discernment": elements,
            "bba_sources": []
        }
        
        # –°–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for i in range(n_sources):
            test_data["bba_sources"].append({
                "id": f"source_{i+1}",
                "bba": minimal_bba
            })
        
        return test_data
    
    def _generate_special_cases(self) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏"""
        print("  ‚úì special: 5 —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤")
        
        special_tests = [
            self._generate_conflict_case(),
            self._generate_agreement_case(),
            self._generate_many_sources_case(),
            self._generate_focused_case(),
            self._generate_diffuse_case()
        ]
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
        validated = []
        for test in special_tests:
            is_valid, errors = DassValidator.validate_data(test)
            if is_valid:
                validated.append(test)
            else:
                print(f"    ‚ö†Ô∏è  –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –Ω–µ–≤–∞–ª–∏–¥–µ–Ω: {errors[0] if errors else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}")
        
        return validated
    
    def _generate_conflict_case(self) -> Dict:
        """–°–ª—É—á–∞–π —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–º"""
        test_data = {
            "metadata": {
                "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏",
                "test_group": "special",
                "test_id": "special_conflict",
                "type": "high_conflict"
            },
            "frame_of_discernment": ["A", "B", "C"],
            "bba_sources": [
                {"id": "source_conflict_1", "bba": {"{A}": 0.9, "{B}": 0.1}},
                {"id": "source_conflict_2", "bba": {"{B}": 0.9, "{C}": 0.1}},
                {"id": "source_conflict_3", "bba": {"{C}": 0.9, "{A}": 0.1}}
            ]
        }
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º BPA
        for source in test_data["bba_sources"]:
            source["bba"] = DassValidator.normalize_bba(source["bba"])
        
        return test_data
    
    def _generate_agreement_case(self) -> Dict:
        """–°–ª—É—á–∞–π —Å –ø–æ–ª–Ω—ã–º —Å–æ–≥–ª–∞—Å–∏–µ–º"""
        test_data = {
            "metadata": {
                "description": "–ü–æ–ª–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏",
                "test_group": "special",
                "test_id": "special_agreement",
                "type": "full_agreement"
            },
            "frame_of_discernment": ["A", "B", "C", "D"],
            "bba_sources": [
                {"id": "source_agree_1", "bba": {"{A}": 0.4, "{B}": 0.3, "{C}": 0.2, "{D}": 0.1}},
                {"id": "source_agree_2", "bba": {"{A}": 0.4, "{B}": 0.3, "{C}": 0.2, "{D}": 0.1}},
                {"id": "source_agree_3", "bba": {"{A}": 0.4, "{B}": 0.3, "{C}": 0.2, "{D}": 0.1}}
            ]
        }
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º BPA
        for source in test_data["bba_sources"]:
            source["bba"] = DassValidator.normalize_bba(source["bba"])
        
        return test_data
    
    def _generate_many_sources_case(self) -> Dict:
        """–ú–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        n_sources = 5  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 10 –¥–æ 5 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        bba_sources = []
        
        base_bba = {"{A}": 0.3, "{B}": 0.3, "{C}": 0.2, "{A,B,C}": 0.2}
        base_bba = DassValidator.normalize_bba(base_bba)
        
        for i in range(n_sources):
            bba_sources.append({
                "id": f"source_many_{i+1}",
                "bba": base_bba
            })
        
        return {
            "metadata": {
                "description": f"–ú–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ({n_sources})",
                "test_group": "special",
                "test_id": "special_many_sources",
                "type": "many_sources"
            },
            "frame_of_discernment": ["A", "B", "C"],
            "bba_sources": bba_sources
        }
    
    def _generate_focused_case(self) -> Dict:
        """–§–æ–∫—É—Å–Ω—ã–µ BPA"""
        test_data = {
            "metadata": {
                "description": "–§–æ–∫—É—Å–Ω—ã–µ BPA (—Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω–æ—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã)",
                "test_group": "special",
                "test_id": "special_focused",
                "type": "focused_bpa"
            },
            "frame_of_discernment": ["A", "B", "C", "D", "E"],
            "bba_sources": [
                {"id": "source_focused_1", "bba": {"{A}": 0.2, "{B}": 0.2, "{C}": 0.2, "{D}": 0.2, "{E}": 0.2}},
                {"id": "source_focused_2", "bba": {"{A}": 0.3, "{B}": 0.2, "{C}": 0.2, "{D}": 0.2, "{E}": 0.1}}
            ]
        }
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º BPA
        for source in test_data["bba_sources"]:
            source["bba"] = DassValidator.normalize_bba(source["bba"])
        
        return test_data
    
    def _generate_diffuse_case(self) -> Dict:
        """–î–∏—Ñ—Ñ—É–∑–Ω—ã–µ BPA"""
        test_data = {
            "metadata": {
                "description": "–î–∏—Ñ—Ñ—É–∑–Ω—ã–µ BPA (–º–Ω–æ–≥–æ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤)",
                "test_group": "special",
                "test_id": "special_diffuse",
                "type": "diffuse_bpa"
            },
            "frame_of_discernment": ["A", "B", "C", "D"],
            "bba_sources": [
                {"id": "source_diffuse_1", "bba": {"{A,B}": 0.25, "{B,C}": 0.25, "{C,D}": 0.25, "{A,D}": 0.25}},
                {"id": "source_diffuse_2", "bba": {"{A,B,C}": 0.3, "{B,C,D}": 0.3, "{A,C,D}": 0.2, "{A,B,D}": 0.2}}
            ]
        }
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º BPA
        for source in test_data["bba_sources"]:
            source["bba"] = DassValidator.normalize_bba(source["bba"])
        
        return test_data


def save_tests(test_suite: Dict[str, List[Dict]], output_dir: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ç–µ—Å—Ç—ã –≤ —Ñ–∞–π–ª—ã"""
    os.makedirs(output_dir, exist_ok=True)
    
    total_saved = 0
    all_tests = []
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ –≥—Ä—É–ø–ø–∞–º
    for group_name, tests in test_suite.items():
        group_dir = os.path.join(output_dir, group_name)
        os.makedirs(group_dir, exist_ok=True)
        
        group_count = 0
        for test in tests:
            test_id = test["metadata"]["test_id"]
            filename = os.path.join(group_dir, f"{test_id}.json")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            is_valid, errors = DassValidator.validate_data(test)
            if not is_valid:
                print(f"    ‚ö†Ô∏è  –¢–µ—Å—Ç {test_id} –Ω–µ–≤–∞–ª–∏–¥–µ–Ω –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {errors[0] if errors else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}")
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            if DassGenerator.save_to_file(test, filename):
                group_count += 1
                total_saved += 1
                all_tests.append(test)
            else:
                print(f"    ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–∞ {test_id}")
        
        print(f"    üíæ {group_name}: {group_count} —Ç–µ—Å—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
    
    return total_saved, all_tests


def create_statistics(all_tests: List[Dict], output_dir: str):
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ—Å—Ç–∞–º"""
    stats = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "total_tests": len(all_tests),
            "description": "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞"
        },
        "by_group": {},
        "by_size": {},
        "quality_metrics": {
            "valid_tests": len(all_tests),
            "invalid_tests": 0,
            "minimal_fallbacks": sum(1 for t in all_tests if t["metadata"].get("is_minimal_fallback", False))
        }
    }
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º
    groups = {}
    sizes = {}
    
    for test in all_tests:
        group = test["metadata"]["test_group"]
        n_elements = len(test["frame_of_discernment"])
        
        # –ü–æ –≥—Ä—É–ø–ø–∞–º
        if group not in groups:
            groups[group] = 0
        groups[group] += 1
        
        # –ü–æ —Ä–∞–∑–º–µ—Ä—É
        if n_elements not in sizes:
            sizes[n_elements] = 0
        sizes[n_elements] += 1
    
    stats["by_group"] = groups
    stats["by_size"] = sizes
    
    # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    if all_tests:
        total_elements = sum(len(test["frame_of_discernment"]) for test in all_tests)
        total_sources = sum(len(test["bba_sources"]) for test in all_tests)
        
        stats["complexity_analysis"] = {
            "avg_elements": total_elements / len(all_tests),
            "avg_sources": total_sources / len(all_tests),
            "max_elements": max(len(test["frame_of_discernment"]) for test in all_tests),
            "min_elements": min(len(test["frame_of_discernment"]) for test in all_tests),
        }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats_file = os.path.join(output_dir, "statistics.json")
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    return stats


def save_last_generation_path(output_dir: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤"""
    last_gen_file = "data/generated/last_generation.txt"
    
    os.makedirs(os.path.dirname(last_gen_file), exist_ok=True)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è –ø–∞–ø–∫–∏
    folder_name = os.path.basename(output_dir)
    
    with open(last_gen_file, 'w', encoding='utf-8') as f:
        f.write(folder_name)


def get_last_generation_path() -> str | None:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤"""
    last_gen_file = "data/generated/last_generation.txt"
    
    if os.path.exists(last_gen_file):
        with open(last_gen_file, 'r', encoding='utf-8') as f:
            folder_name = f.read().strip()
        
        return os.path.join("data/generated", folder_name)
    
    return None


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üî¨ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ì–ï–ù–ï–†–ê–¢–û–† –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•")
    print("–ë—ã—Å—Ç—Ä–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 30+ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö, –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤")
    print("=" * 70)
    
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –ø–∞–ø–∫–∏
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"data/generated/tests_{timestamp}"
    
    print(f"üìÅ –¢–µ—Å—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã
    generator = OptimizedTestDataGenerator()
    test_suite = generator.generate_optimized_test_suite()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤...")
    total_saved, all_tests = save_tests(test_suite, output_dir)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìà –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
    stats = create_statistics(all_tests, output_dir)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    save_last_generation_path(output_dir)
    
    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω–æ {total_saved} –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {stats['metadata']['total_tests']}")
    print(f"  –í–∞–ª–∏–¥–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {stats['quality_metrics']['valid_tests']}")
    print(f"  –†–µ–∑–µ—Ä–≤–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {stats['quality_metrics']['minimal_fallbacks']}")
    
    print(f"\n  –ü–æ –≥—Ä—É–ø–ø–∞–º:")
    for group, count in sorted(stats["by_group"].items()):
        print(f"    {group:10}: {count:3} —Ç–µ—Å—Ç–æ–≤")
    
    print(f"\n  –ü–æ —Ä–∞–∑–º–µ—Ä—É —Ñ—Ä–µ–π–º–∞:")
    for size in sorted(stats["by_size"].keys()):
        count = stats["by_size"][size]
        print(f"    {size:2} —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {count:3} —Ç–µ—Å—Ç–æ–≤")
    
    if "complexity_analysis" in stats:
        complexity = stats["complexity_analysis"]
        print(f"\n  –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:")
        print(f"    –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ñ—Ä–µ–π–º–∞: {complexity['avg_elements']:.1f} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        print(f"    –°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {complexity['avg_sources']:.1f}")
        print(f"    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ—Ä–µ–π–º–∞: {complexity['max_elements']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    last_gen = get_last_generation_path()
    if last_gen:
        print(f"\nüìÅ –î–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:")
        print(f"  python scripts/profile_benchmark.py --library our --tests last --profiling off")
        print(f"  python scripts/profile_benchmark.py --library our --tests {last_gen} --profiling off")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
