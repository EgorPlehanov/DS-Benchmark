#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –î–µ–º–ø—Å—Ç–µ—Ä–∞-–®–µ–π—Ñ–µ—Ä–∞
–°–æ–∑–¥–∞–µ—Ç –ø–æ 10 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
"""

import os
import json
import random
from typing import List, Dict, Any
from datetime import datetime
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.generators.dass_generator import DassGenerator
from src.generators.validator import DassValidator


def generate_small_tests() -> List[Dict[str, Any]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 10 –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ (3-4 —ç–ª–µ–º–µ–Ω—Ç–∞, 2-3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞)"""
    tests = []
    
    # –ë–∞–∑–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤
    base_elements = ["A", "B", "C", "D"]
    
    for i in range(1, 11):  # 10 —Ç–µ—Å—Ç–æ–≤
        # –í–∞—Ä–∏–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Ñ—Ä–µ–π–º–∞: 3 –∏–ª–∏ 4 —ç–ª–µ–º–µ–Ω—Ç–∞
        n_elements = random.choice([3, 4])
        elements = base_elements[:n_elements]
        
        # –í–∞—Ä–∏–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: 2 –∏–ª–∏ 3
        n_sources = random.choice([2, 3])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç
        test_data = DassGenerator.generate_simple(
            elements=elements,
            n_sources=n_sources,
            density=0.2,  # –Ω–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
            include_empty=random.choice([True, False])
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        test_data["metadata"]["description"] = (
            f"–ú–∞–ª–µ–Ω—å–∫–∏–π —Ç–µ—Å—Ç #{i}: {n_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, {n_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
        )
        test_data["metadata"]["test_group"] = "small"
        test_data["metadata"]["test_id"] = f"small_{i:02d}"
        
        tests.append(test_data)
    
    return tests


def generate_medium_tests() -> List[Dict[str, Any]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 10 —Å—Ä–µ–¥–Ω–∏—Ö —Ç–µ—Å—Ç–æ–≤ (5-6 —ç–ª–µ–º–µ–Ω—Ç–æ–≤, 3-4 –∏—Å—Ç–æ—á–Ω–∏–∫–∞)"""
    tests = []
    
    # –ë–∞–∑–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö —Ç–µ—Å—Ç–æ–≤
    base_elements = ["A", "B", "C", "D", "E", "F"]
    
    for i in range(1, 11):  # 10 —Ç–µ—Å—Ç–æ–≤
        # –í–∞—Ä–∏–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Ñ—Ä–µ–π–º–∞: 5 –∏–ª–∏ 6 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        n_elements = random.choice([5, 6])
        elements = base_elements[:n_elements]
        
        # –í–∞—Ä–∏–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: 3 –∏–ª–∏ 4
        n_sources = random.choice([3, 4])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç
        test_data = DassGenerator.generate_simple(
            elements=elements,
            n_sources=n_sources,
            density=0.15,  # —Å—Ä–µ–¥–Ω—è—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
            include_empty=random.choice([True, False])
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        test_data["metadata"]["description"] = (
            f"–°—Ä–µ–¥–Ω–∏–π —Ç–µ—Å—Ç #{i}: {n_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, {n_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
        )
        test_data["metadata"]["test_group"] = "medium"
        test_data["metadata"]["test_id"] = f"medium_{i:02d}"
        
        tests.append(test_data)
    
    return tests


def generate_large_tests() -> List[Dict[str, Any]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 10 –±–æ–ª—å—à–∏—Ö —Ç–µ—Å—Ç–æ–≤ (7-8 —ç–ª–µ–º–µ–Ω—Ç–æ–≤, 4-5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)"""
    tests = []
    
    # –ë–∞–∑–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ—Å—Ç–æ–≤
    base_elements = ["A", "B", "C", "D", "E", "F", "G", "H"]
    
    for i in range(1, 11):  # 10 —Ç–µ—Å—Ç–æ–≤
        # –í–∞—Ä–∏–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Ñ—Ä–µ–π–º–∞: 7 –∏–ª–∏ 8 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        n_elements = random.choice([7, 8])
        elements = base_elements[:n_elements]
        
        # –í–∞—Ä–∏–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: 4 –∏–ª–∏ 5
        n_sources = random.choice([4, 5])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç
        test_data = DassGenerator.generate_simple(
            elements=elements,
            n_sources=n_sources,
            density=0.1,  # –Ω–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (–∏–Ω–∞—á–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π)
            include_empty=random.choice([True, False])
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        test_data["metadata"]["description"] = (
            f"–ë–æ–ª—å—à–æ–π —Ç–µ—Å—Ç #{i}: {n_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, {n_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
        )
        test_data["metadata"]["test_group"] = "large"
        test_data["metadata"]["test_id"] = f"large_{i:02d}"
        
        tests.append(test_data)
    
    return tests


def validate_and_save_tests(tests: List[Dict[str, Any]], output_dir: str):
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ—Å—Ç—ã –≤ —Ñ–∞–π–ª—ã"""
    os.makedirs(output_dir, exist_ok=True)
    
    saved_count = 0
    for test_data in tests:
        test_id = test_data["metadata"]["test_id"]
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º
        is_valid, errors = DassValidator.validate_data(test_data)
        if not is_valid:
            print(f"  ‚ö†Ô∏è  –¢–µ—Å—Ç {test_id} –Ω–µ–≤–∞–ª–∏–¥–µ–Ω: {errors[:1]}...")
            continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        filename = os.path.join(output_dir, f"{test_id}.json")
        if DassGenerator.save_to_file(test_data, filename):
            saved_count += 1
            print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω: {test_id}.json")
    
    return saved_count


def generate_all_tests():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    print("üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤")
    print("=" * 50)
    
    # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    output_dir = "data/generated/tests"
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –≥—Ä—É–ø–ø—ã —Ç–µ—Å—Ç–æ–≤
    print("\nüìä 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ (3-4 —ç–ª–µ–º–µ–Ω—Ç–∞, 2-3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞)...")
    small_tests = generate_small_tests()
    
    print("\nüìä 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–µ–¥–Ω–∏—Ö —Ç–µ—Å—Ç–æ–≤ (5-6 —ç–ª–µ–º–µ–Ω—Ç–æ–≤, 3-4 –∏—Å—Ç–æ—á–Ω–∏–∫–∞)...")
    medium_tests = generate_medium_tests()
    
    print("\nüìä 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö —Ç–µ—Å—Ç–æ–≤ (7-8 —ç–ª–µ–º–µ–Ω—Ç–æ–≤, 4-5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)...")
    large_tests = generate_large_tests()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
    
    total_saved = 0
    total_saved += validate_and_save_tests(small_tests, output_dir)
    total_saved += validate_and_save_tests(medium_tests, output_dir)
    total_saved += validate_and_save_tests(large_tests, output_dir)
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å–Ω—ã–π —Ñ–∞–π–ª
    create_index_file(output_dir, small_tests + medium_tests + large_tests)
    
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω–æ {total_saved} —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ {output_dir}/")
    print("\nüìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤:")
    print(f"  {output_dir}/small_01.json ... small_10.json  (10 —Ñ–∞–π–ª–æ–≤)")
    print(f"  {output_dir}/medium_01.json ... medium_10.json (10 —Ñ–∞–π–ª–æ–≤)")
    print(f"  {output_dir}/large_01.json ... large_10.json  (10 —Ñ–∞–π–ª–æ–≤)")
    print(f"  {output_dir}/index.json     (–∏–Ω–¥–µ–∫—Å –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤)")


def create_index_file(output_dir: str, all_tests: List[Dict[str, Any]]):
    """–°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å–Ω—ã–π —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ —Ç–µ—Å—Ç–∞–º–∏"""
    index = {
        "metadata": {
            "format": "DASS-INDEX",
            "version": "1.0",
            "description": "–ò–Ω–¥–µ–∫—Å –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤",
            "generated_at": datetime.now().isoformat(),
            "total_tests": len(all_tests)
        },
        "tests": []
    }
    
    for test in all_tests:
        test_info = {
            "test_id": test["metadata"]["test_id"],
            "group": test["metadata"]["test_group"],
            "description": test["metadata"]["description"],
            "n_elements": len(test["frame_of_discernment"]),
            "n_sources": len(test["bba_sources"]),
            "filename": f"{test['metadata']['test_id']}.json"
        }
        index["tests"].append(test_info)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
    index_file = os.path.join(output_dir, "index.json")
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(index, f, indent=2, ensure_ascii=False)
    
    print(f"  ‚úì –°–æ–∑–¥–∞–Ω –∏–Ω–¥–µ–∫—Å: index.json")


if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    print(f"–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã
    generate_all_tests()