"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ä–∞–Ω–Ω–µ—Ä –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π —Ç–µ–æ—Ä–∏–∏ –î–µ–º–ø—Å—Ç–µ—Ä–∞-–®–µ–π—Ñ–µ—Ä–∞.
–í—ã–ø–æ–ª–Ω—è–µ—Ç 4-—à–∞–≥–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""

import os
import json
import time
import tracemalloc
import statistics
import sys
import psutil
from typing import Dict, List, Any, Optional, Tuple, Callable
from datetime import datetime
from pathlib import Path

from ..adapters.base_adapter import BaseDempsterShaferAdapter
from ..profiling.artifacts import ArtifactManager


class UniversalBenchmarkRunner:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ä–∞–Ω–Ω–µ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ —Ç–µ–æ—Ä–∏–∏ –î–®.
    
    –í—ã–ø–æ–ª–Ω—è–µ—Ç:
    1. –ó–∞–≥—Ä—É–∑–∫—É —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    2. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ 4-—à–∞–≥–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
    3. –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    
    def __init__(self, adapter: BaseDempsterShaferAdapter, 
                 results_dir: str = "results/profiling"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–Ω–Ω–µ—Ä–∞.
        
        Args:
            adapter: –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä—É–µ–º–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
            results_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        self.adapter = adapter
        self.adapter_name = adapter.benchmark_name
        self.results_dir = results_dir
        self.results = []
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: results/profiling/<library>/<timestamp>/
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.artifact_manager = ArtifactManager(
            base_dir=results_dir,
            adapter_name=self.adapter_name,
            run_id=self.run_id,
        )
        self.run_dir = str(self.artifact_manager.run_dir)


        print(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Ä–∞–Ω–Ω–µ—Ä –¥–ª—è {self.adapter_name}")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.run_dir}")

    def set_run_parameters(self, **parameters: Any) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞ —Ç–æ–ª—å–∫–æ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª run_parameters.json."""
        self.artifact_manager.save_run_parameters(parameters)

    def _supports_cr(self) -> bool:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ stdout –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç carriage return."""
        return hasattr(sys.stdout, "isatty") and sys.stdout.isatty()

    def _render_inline_progress(self, text: str) -> None:
        """–ü–µ—á–∞—Ç–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–µ –∏–ª–∏ fallback-—Å—Ç—Ä–æ–∫–æ–π."""
        if self._supports_cr():
            print(f"\r{text}", end="", flush=True)
            return
        print(text)

    def _finish_inline_progress(self) -> None:
        """–ó–∞–≤–µ—Ä—à–∞–µ—Ç inline-–ø–µ—á–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥–æ–º —Å—Ç—Ä–æ–∫–∏."""
        if self._supports_cr():
            print()
    
    def run_test(self, test_data: Dict[str, Any], 
             test_name: str,
             iterations: int = 3,
             alphas: Optional[List[float]] = None) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —Ç–µ—Å—Ç.
        """
        print(f"\nüß™ –¢–µ—Å—Ç: {test_name} (–∏—Ç–µ—Ä–∞—Ü–∏–π: {iterations})")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        test_results = {
            "metadata": {
                "test_name": test_name,  # ‚úÖ –°–û–•–†–ê–ù–Ø–ï–ú –ò–ú–Ø –¢–ï–°–¢–ê
                "adapter": self.adapter_name,
                "iterations": iterations,
                "timestamp": datetime.now().isoformat(),
                "frame_size": len(test_data.get("frame_of_discernment", [])),
                "sources_count": len(test_data.get("bba_sources", []))
            },
            "iterations": [],
            "aggregated": {}
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä
        loaded_data = self.adapter.load_from_dass(test_data)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ö–æ–¥ —Ç–µ—Å—Ç–∞ –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç
        self.artifact_manager.save_test_input(test_data, test_name)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        if alphas is None:
            sources_count = self.adapter.get_sources_count(loaded_data)
            alphas = [0.1] * sources_count
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏
        for i in range(iterations):
            self._render_inline_progress(f"   ‚Üª –ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}/{iterations}")

            iteration_results = self._run_single_iteration(
                loaded_data=loaded_data,
                test_data=test_data,
                iteration_num=i+1,
                alphas=alphas,
                test_name=test_name  # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –ò–ú–Ø –¢–ï–°–¢–ê
            )
            
            test_results["iterations"].append(iteration_results)
            self._render_inline_progress(f"   ‚úÖ –ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}/{iterations}")
            self._finish_inline_progress()
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        test_results["aggregated"] = self._aggregate_iteration_results(
            test_results["iterations"]
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._save_test_results(test_results, test_name)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.results.append(test_results)
        
        return test_results
    
    def _run_single_iteration(self, 
                         loaded_data: Any,
                         test_data: Dict[str, Any],
                         iteration_num: int,
                         alphas: List[float],
                         test_name: str = "") -> Dict[str, Any]:  # ‚úÖ –î–û–ë–ê–í–õ–Ø–ï–ú test_name
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é —Ç–µ—Å—Ç–∞.
        """
        iteration_results = {
            "iteration": iteration_num,
            "performance": {}
        }
        
        # === –®–ê–ì 1: –ò—Å—Ö–æ–¥–Ω—ã–µ Belief/Plausibility ===
        step1_results, step1_metrics = self._measure_performance(
            self._execute_step1,
            loaded_data,
            step_name="step1_original",
            test_name=test_name,           # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –ò–ú–Ø –¢–ï–°–¢–ê
            iteration=iteration_num        # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –ù–û–ú–ï–† –ò–¢–ï–†–ê–¶–ò–ò
        )
        iteration_results["step1"] = step1_results
        iteration_results["performance"]["step1"] = step1_metrics
        
        # === –®–ê–ì 2: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –î–µ–º–ø—Å—Ç–µ—Ä–æ–º ===
        step2_results, step2_metrics = self._measure_performance(
            self._execute_step2,
            loaded_data,
            step_name="step2_dempster",
            test_name=test_name,           # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –ò–ú–Ø –¢–ï–°–¢–ê
            iteration=iteration_num        # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –ù–û–ú–ï–† –ò–¢–ï–†–ê–¶–ò–ò
        )
        iteration_results["step2"] = step2_results
        iteration_results["performance"]["step2"] = step2_metrics
        
        # === –®–ê–ì 3: –î–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ + –î–µ–º–ø—Å—Ç–µ—Ä ===
        step3_results, step3_metrics = self._measure_performance(
            self._execute_step3,
            loaded_data,
            alphas,
            step_name="step3_discount_dempster",
            test_name=test_name,           # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –ò–ú–Ø –¢–ï–°–¢–ê
            iteration=iteration_num        # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –ù–û–ú–ï–† –ò–¢–ï–†–ê–¶–ò–ò
        )
        iteration_results["step3"] = step3_results
        iteration_results["performance"]["step3"] = step3_metrics
        
        # === –®–ê–ì 4: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ø–≥–µ—Ä–æ–º ===
        step4_results, step4_metrics = self._measure_performance(
            self._execute_step4,
            loaded_data,
            step_name="step4_yager",
            test_name=test_name,           # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –ò–ú–Ø –¢–ï–°–¢–ê
            iteration=iteration_num        # ‚úÖ –ü–ï–†–ï–î–ê–ï–ú –ù–û–ú–ï–† –ò–¢–ï–†–ê–¶–ò–ò
        )
        iteration_results["step4"] = step4_results
        iteration_results["performance"]["step4"] = step4_metrics
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Ç–µ—Ä–∞—Ü–∏–∏
        iteration_results["performance"]["total"] = {
            "time_total_ms": sum(
                step["time_ms"] for step in iteration_results["performance"].values() 
                if isinstance(step, dict) and "time_ms" in step
            ),
            "memory_peak_mb": max(
                step.get("memory_peak_mb", 0) for step in iteration_results["performance"].values()
                if isinstance(step, dict)
            )
        }
        
        return iteration_results
    
    def _execute_step1(self, loaded_data: Any) -> Dict[str, Any]:
        """–®–∞–≥ 1: –ò—Å—Ö–æ–¥–Ω—ã–µ Belief/Plausibility –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        frame_elements = self.adapter.get_frame_of_discernment(loaded_data)
        sources_count = self.adapter.get_sources_count(loaded_data)
        
        results = {
            "frame_elements": frame_elements,
            "sources": []
        }
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤—ã—á–∏—Å–ª—è–µ–º Belief –∏ Plausibility
        for i in range(sources_count):
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            source_data = self._get_source_data(loaded_data, i)
            
            source_results = {
                "source_id": f"source_{i+1}",
                "beliefs": {},
                "plausibilities": {}
            }
            
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
            for element in frame_elements:
                belief = self.adapter.calculate_belief(source_data, element)
                plausibility = self.adapter.calculate_plausibility(source_data, element)
                
                source_results["beliefs"][f"{{{element}}}"] = belief
                source_results["plausibilities"][f"{{{element}}}"] = plausibility
            
            # –î–ª—è –≤—Å–µ–≥–æ —Ñ—Ä–µ–π–º–∞ (Œ©)
            omega = "{" + ",".join(sorted(frame_elements)) + "}"
            source_results["beliefs"][omega] = self.adapter.calculate_belief(source_data, frame_elements)  # Bel(Œ©) = 1.0
            source_results["plausibilities"][omega] = self.adapter.calculate_plausibility(source_data, frame_elements)  # Pl(Œ©) = 1.0

            results["sources"].append(source_results)
        
        return results
    
    def _execute_step2(self, loaded_data: Any) -> Dict[str, Any]:
        """–®–∞–≥ 2: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –ø—Ä–∞–≤–∏–ª—É –î–µ–º–ø—Å—Ç–µ—Ä–∞"""
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        combined_bpa_str = self.adapter.combine_sources_dempster(loaded_data)
        
        # –ö–û–ù–í–ï–†–¢–ò–†–£–ï–ú —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –Ω–∞—à–µ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞
        combined_bpa = self._convert_string_bpa_to_frozenset(combined_bpa_str)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º BPA –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è Belief/Plausibility
        combined_data = self._create_combined_data(loaded_data, combined_bpa)
        
        frame_elements = self.adapter.get_frame_of_discernment(loaded_data)
        
        results = {
            "combined_bpa": combined_bpa_str,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å—Ç—Ä–æ–∫–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            "beliefs": {},
            "plausibilities": {}
        }
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        for element in frame_elements:
            belief = self.adapter.calculate_belief(combined_data, element)
            plausibility = self.adapter.calculate_plausibility(combined_data, element)
            
            results["beliefs"][f"{{{element}}}"] = belief
            results["plausibilities"][f"{{{element}}}"] = plausibility
        
        # –î–ª—è –≤—Å–µ–≥–æ —Ñ—Ä–µ–π–º–∞
        omega = "{" + ",".join(sorted(frame_elements)) + "}"
        results["beliefs"][omega] = self.adapter.calculate_belief(combined_data, frame_elements)
        results["plausibilities"][omega] = self.adapter.calculate_plausibility(combined_data, frame_elements)
        
        return results
    
    def _execute_step3(self, loaded_data: Any, alphas: List[float]) -> Dict[str, Any]:
        """–®–∞–≥ 3: –î–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ + –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –î–µ–º–ø—Å—Ç–µ—Ä–æ–º"""
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources_count = self.adapter.get_sources_count(loaded_data)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫ –∫–∞–∂–¥–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É —Å –µ–≥–æ alpha
        discounted_bpas_str = []
        for i in range(sources_count):
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            source_data = self._get_source_data(loaded_data, i)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å alpha –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            alpha = alphas[i] if i < len(alphas) else 0.1
            
            # –î–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫ –æ–¥–Ω–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É
            discounted_list = self.adapter.apply_discounting(source_data, alpha)
            
            if discounted_list and len(discounted_list) > 0:
                discounted_bpas_str.append(discounted_list[0])
            else:
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π BPA
                original_bpa = self._extract_bpa_from_source(loaded_data, i)
                discounted_bpas_str.append(original_bpa)
        
        # –ö–û–ù–í–ï–†–¢–ò–†–£–ï–ú —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç frozenset
        discounted_bpas = [self._convert_string_bpa_to_frozenset(bpa_str) 
                          for bpa_str in discounted_bpas_str]
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Å –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        discounted_data = self._create_discounted_data(loaded_data, discounted_bpas)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        combined_bpa_str = self.adapter.combine_sources_dempster(discounted_data)
        combined_bpa = self._convert_string_bpa_to_frozenset(combined_bpa_str)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º BPA
        combined_data = self._create_combined_data(discounted_data, combined_bpa)
        
        frame_elements = self.adapter.get_frame_of_discernment(loaded_data)
        
        results = {
            "discounted_bpas": discounted_bpas_str,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å—Ç—Ä–æ–∫–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            "combined_bpa": combined_bpa_str,
            "beliefs": {},
            "plausibilities": {}
        }
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        for element in frame_elements:
            belief = self.adapter.calculate_belief(combined_data, element)
            plausibility = self.adapter.calculate_plausibility(combined_data, element)
            
            results["beliefs"][f"{{{element}}}"] = belief
            results["plausibilities"][f"{{{element}}}"] = plausibility
        
        # –î–ª—è –≤—Å–µ–≥–æ —Ñ—Ä–µ–π–º–∞
        omega = "{" + ",".join(sorted(frame_elements)) + "}"
        results["beliefs"][omega] = self.adapter.calculate_belief(combined_data, frame_elements)
        results["plausibilities"][omega] = self.adapter.calculate_plausibility(combined_data, frame_elements)
        
        return results
    
    def _execute_step4(self, loaded_data: Any) -> Dict[str, Any]:
        """–®–∞–≥ 4: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –ø—Ä–∞–≤–∏–ª—É –Ø–≥–µ—Ä–∞"""
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –Ø–≥–µ—Ä—É
        combined_bpa_str = self.adapter.combine_sources_yager(loaded_data)
        
        # –ö–û–ù–í–ï–†–¢–ò–†–£–ï–ú —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç frozenset
        combined_bpa = self._convert_string_bpa_to_frozenset(combined_bpa_str)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º BPA
        combined_data = self._create_combined_data(loaded_data, combined_bpa)
        
        frame_elements = self.adapter.get_frame_of_discernment(loaded_data)
        
        results = {
            "combined_bpa": combined_bpa_str,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å—Ç—Ä–æ–∫–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            "beliefs": {},
            "plausibilities": {}
        }
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        for element in frame_elements:
            belief = self.adapter.calculate_belief(combined_data, element)
            plausibility = self.adapter.calculate_plausibility(combined_data, element)
            
            results["beliefs"][f"{{{element}}}"] = belief
            results["plausibilities"][f"{{{element}}}"] = plausibility
        
        # –î–ª—è –≤—Å–µ–≥–æ —Ñ—Ä–µ–π–º–∞
        omega = "{" + ",".join(sorted(frame_elements)) + "}"
        results["beliefs"][omega] = self.adapter.calculate_belief(combined_data, frame_elements)
        results["plausibilities"][omega] = self.adapter.calculate_plausibility(combined_data, frame_elements)
        
        return results
    
    def _convert_string_bpa_to_frozenset(self, bpa_str: Dict[str, float]) -> Dict[frozenset, float]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç BPA –∏–∑ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç frozenset."""
        if not bpa_str:
            return {}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ—Ç —É–∂–µ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        first_key = next(iter(bpa_str.keys()))
        if isinstance(first_key, frozenset):
            return bpa_str # type: ignore
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ –≤–æ frozenset
        bpa_frozenset: Dict[frozenset, float] = {}
        for subset_str, mass in bpa_str.items():
            if subset_str == "{}":
                subset = frozenset()
            else:
                elements = subset_str.strip("{}").split(",")
                if elements == ['']:
                    subset = frozenset()
                else:
                    subset = frozenset(elements)
            bpa_frozenset[subset] = mass
        
        return bpa_frozenset
        
    def _is_full_conflict_message(self, message: str) -> bool:
        lowered = str(message).lower()
        return any(keyword in lowered for keyword in ["–ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç", "full conflict", "k=1.0", "–∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"])

    def _measure_performance(self, func: Callable, *args,
                       step_name: str = "", **kwargs) -> Tuple[Any, Dict[str, Any]]:
        """–ò–∑–º–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å—Ç–∞—Ç—É—Å —ç—Ç–∞–ø–∞."""
        metrics: Dict[str, Any] = {
            "status": "success",
            "supported": True,
        }

        tracemalloc.start()
        snapshot1 = tracemalloc.take_snapshot()

        process = psutil.Process()
        cpu_before = process.cpu_percent(interval=None)
        start_time = time.perf_counter()

        try:
            result = func(*args, **kwargs)
        except NotImplementedError as e:
            metrics["status"] = "not_supported"
            metrics["supported"] = False
            metrics["error_type"] = type(e).__name__
            metrics["error"] = str(e)
            result = {"status": "not_supported", "error": str(e)}
        except ValueError as e:
            error_msg = str(e)
            if self._is_full_conflict_message(error_msg):
                metrics["status"] = "full_conflict"
                metrics["warning"] = "–ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ (K=1.0)"
                metrics["full_conflict"] = True
                result = {"status": "full_conflict", "warning": metrics["warning"]}
            else:
                metrics["status"] = "failed"
                metrics["error_type"] = type(e).__name__
                metrics["error"] = error_msg
                result = {"status": "failed", "error": error_msg}
        except Exception as e:
            metrics["status"] = "failed"
            metrics["error_type"] = type(e).__name__
            metrics["error"] = str(e)
            result = {"status": "failed", "error": str(e)}

        end_time = time.perf_counter()
        cpu_after = process.cpu_percent(interval=None)
        snapshot2 = tracemalloc.take_snapshot()
        tracemalloc.stop()

        metrics["time_ms"] = (end_time - start_time) * 1000
        memory_stats = snapshot2.compare_to(snapshot1, 'lineno')
        memory_usage = sum(stat.size for stat in memory_stats)
        metrics["memory_peak_mb"] = memory_usage / 1024 / 1024
        metrics["cpu_usage_percent"] = max(0, cpu_after - cpu_before)

        return result, metrics
    
    def _get_source_data(self, loaded_data: Any, source_index: int) -> Any:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –Ω–∞—à–µ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞
        if isinstance(loaded_data, dict) and 'bpas' in loaded_data:
            # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ —Å –æ–¥–Ω–∏–º BPA
            single_source_data = loaded_data.copy()
            if source_index < len(loaded_data['bpas']):
                single_source_data['bpas'] = [loaded_data['bpas'][source_index]]
            else:
                single_source_data['bpas'] = [{}]
            return single_source_data
        
        # –ï—Å–ª–∏ –∞–¥–∞–ø—Ç–µ—Ä –∏–º–µ–µ—Ç –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
        return loaded_data
    
    def _extract_bpa_from_source(self, loaded_data: Any, source_index: int) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç BPA –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
        source_data = self._get_source_data(loaded_data, source_index)
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å BPA
        if isinstance(source_data, dict) and 'bpas' in source_data and source_data['bpas']:
            bpa = source_data['bpas'][0]
            # –ï—Å–ª–∏ BPA –≤ —Ñ–æ—Ä–º–∞—Ç–µ frozenset, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
            if bpa and isinstance(next(iter(bpa.keys())), frozenset):
                return self._convert_frozenset_bpa_to_string(bpa)
        
        return {}
    
    def _convert_frozenset_bpa_to_string(self, bpa_frozenset: Dict[frozenset, float]) -> Dict[str, float]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç BPA –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ frozenset –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç."""
        if not bpa_frozenset:
            return {}
        
        bpa_str = {}
        for subset, mass in bpa_frozenset.items():
            if not subset:
                subset_str = "{}"
            else:
                subset_str = "{" + ",".join(sorted(subset)) + "}"
            bpa_str[subset_str] = mass
        
        return bpa_str
    
    def _create_combined_data(self, original_data: Any, 
                            combined_bpa: Dict[frozenset, float]) -> Any:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç –¥–∞–Ω–Ω—ã—Ö —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º BPA."""
        if isinstance(original_data, dict):
            combined_data = original_data.copy()
            combined_data['bpas'] = [combined_bpa]
            return combined_data
        
        return original_data
    
    def _create_discounted_data(self, original_data: Any,
                              discounted_bpas: List[Dict[frozenset, float]]) -> Any:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç –¥–∞–Ω–Ω—ã—Ö —Å –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ BPA."""
        if isinstance(original_data, dict):
            discounted_data = original_data.copy()
            discounted_data['bpas'] = discounted_bpas
            return discounted_data
        
        return original_data
    
    def _aggregate_iteration_results(self, iterations: List[Dict]) -> Dict[str, Any]:
        """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π."""
        if not iterations:
            return {}
        
        aggregated = {
            "performance": {}
        }
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        for step in ["step1", "step2", "step3", "step4"]:
            step_times = [
                iteration["performance"][step]["time_ms"]
                for iteration in iterations
                if step in iteration.get("performance", {}) and "error" not in iteration["performance"][step]
            ]
            
            if step_times:
                aggregated["performance"][step] = {
                    "time_ms": {
                        "min": min(step_times),
                        "max": max(step_times),
                        "mean": statistics.mean(step_times),
                        "median": statistics.median(step_times),
                        "std": statistics.stdev(step_times) if len(step_times) > 1 else 0
                    }
                }
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
        total_times = [
            iteration["performance"]["total"]["time_total_ms"]
            for iteration in iterations
            if "total" in iteration.get("performance", {})
        ]
        
        if total_times:
            aggregated["performance"]["total"] = {
                "time_total_ms": {
                    "min": min(total_times),
                    "max": max(total_times),
                    "mean": statistics.mean(total_times),
                    "median": statistics.median(total_times),
                    "std": statistics.stdev(total_times) if len(total_times) > 1 else 0
                }
            }
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        if iterations:
            aggregated["results"] = iterations[-1]
        
        return aggregated
    
    def _save_test_results(self, test_results: Dict[str, Any], test_name: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∞."""
        self.artifact_manager.save_test_results(test_results, test_name)

    def _classify_test_status(self, test_result: Dict[str, Any]) -> str:
        if test_result.get("error"):
            return "failed"

        statuses: list[str] = []
        for iteration in test_result.get("iterations", []):
            perf = iteration.get("performance", {})
            for step in ("step1", "step2", "step3", "step4"):
                step_perf = perf.get(step, {})
                statuses.append(step_perf.get("status", "success"))

        if any(status == "failed" for status in statuses):
            return "failed"
        if any(status == "full_conflict" for status in statuses):
            return "full_conflict"
        if statuses and all(status == "not_supported" for status in statuses):
            return "not_supported"
        return "success"

    def _create_run_summary(self, discovered_tests: int) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –µ–¥–∏–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∑–∞–ø—É—Å–∫—É —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π/–ø–æ–≤—Ç–æ—Ä–æ–≤."""
        step_map = {
            "step1": "step1_original",
            "step2": "step2_dempster",
            "step3": "step3_discount_dempster",
            "step4": "step4_yager",
        }

        run_summary: Dict[str, Any] = {
            "run_meta": {
                "adapter": self.adapter_name,
                "run_id": self.run_id,
                "generated_at": datetime.now().isoformat(),
                "discovered_tests": discovered_tests,
                "executed_tests": len(self.results),
                "run_dir": self.run_dir,
            },
            "totals": {
                "success": 0,
                "failed": 0,
                "full_conflict": 0,
                "not_supported": 0,
            },
            "tests": [],
            "statistics": {
                "steps": {},
                "total_time_ms": {"sample_count": 0, "mean": 0.0, "min": 0.0, "max": 0.0, "median": 0.0, "std": 0.0},
                "total_time_per_repeat_ms": {"sample_count": 0, "mean": 0.0, "min": 0.0, "max": 0.0, "median": 0.0, "std": 0.0},
                "errors": {},
            },
        }

        step_total_samples: Dict[str, list[float]] = {step: [] for step in step_map}
        step_normalized_samples: Dict[str, list[float]] = {step: [] for step in step_map}
        step_counters: Dict[str, Dict[str, int]] = {
            step: {"applicable": 0, "success": 0, "failed": 0, "full_conflict": 0, "not_supported": 0}
            for step in step_map
        }
        total_times: list[float] = []
        total_per_repeat_times: list[float] = []

        def _stats(values: list[float]) -> Dict[str, float]:
            if not values:
                return {"sample_count": 0, "mean": 0.0, "min": 0.0, "max": 0.0, "median": 0.0, "std": 0.0}
            return {
                "sample_count": len(values),
                "mean": statistics.mean(values),
                "min": min(values),
                "max": max(values),
                "median": statistics.median(values),
                "std": statistics.stdev(values) if len(values) > 1 else 0.0,
            }

        for test_result in self.results:
            metadata = test_result.get("metadata", {})
            test_name = metadata.get("test_name", "unknown")
            iterations = test_result.get("iterations", [])

            test_entry: Dict[str, Any] = {
                "test_name": test_name,
                "status": self._classify_test_status(test_result),
                "frame_size": metadata.get("frame_size"),
                "sources_count": metadata.get("sources_count"),
                "input_ref": f"input/{test_name}_input.json",
                "result_ref": f"test_results/{test_name}_results.json",
                "iterations_count": len(iterations),
                "steps": {},
                "errors": [],
            }

            test_step_total_samples: Dict[str, list[float]] = {step: [] for step in step_map}
            test_step_normalized_samples: Dict[str, list[float]] = {step: [] for step in step_map}
            test_total_samples: list[float] = []
            test_total_per_repeat_samples: list[float] = []

            for iteration in iterations:
                perf = iteration.get("performance", {})
                successful_step_total_times: list[float] = []
                successful_step_normalized_times: list[float] = []

                for step_key, step_name in step_map.items():
                    step_perf = perf.get(step_key, {})
                    status = step_perf.get("status", "success")
                    supported = step_perf.get("supported", status != "not_supported")
                    time_total_ms = float(step_perf.get("time_ms", 0.0) or 0.0)
                    repeat_count = int(step_perf.get("step_repeat_count", metadata.get("step_repeat_count", 1)) or 1)
                    time_per_repeat_ms = float(
                        step_perf.get("time_per_repeat_ms", time_total_ms / max(1, repeat_count))
                    )

                    counters = step_counters[step_key]
                    counters[status if status in counters else "failed"] += 1
                    if status != "not_supported":
                        counters["applicable"] += 1

                    if status == "success":
                        step_total_samples[step_key].append(time_total_ms)
                        step_normalized_samples[step_key].append(time_per_repeat_ms)
                        test_step_total_samples[step_key].append(time_total_ms)
                        test_step_normalized_samples[step_key].append(time_per_repeat_ms)
                        successful_step_total_times.append(time_total_ms)
                        successful_step_normalized_times.append(time_per_repeat_ms)
                    else:
                        error_message = step_perf.get("error") or step_perf.get("warning")
                        if error_message:
                            err_key = f"{step_name}: {error_message}"
                            run_summary["statistics"]["errors"].setdefault(err_key, []).append(test_name)
                            test_entry["errors"].append({
                                "iteration": iteration.get("iteration") or iteration.get("run"),
                                "step": step_name,
                                "status": status,
                                "message": error_message,
                            })

                if len(successful_step_total_times) == 4:
                    test_total = sum(successful_step_total_times)
                    test_total_per_repeat = sum(successful_step_normalized_times)
                    total_times.append(test_total)
                    total_per_repeat_times.append(test_total_per_repeat)
                    test_total_samples.append(test_total)
                    test_total_per_repeat_samples.append(test_total_per_repeat)

            for step_key, step_name in step_map.items():
                test_entry["steps"][step_key] = {
                    "name": step_name,
                    "samples": {
                        "time_total_ms": _stats(test_step_total_samples[step_key]),
                        "time_per_repeat_ms": _stats(test_step_normalized_samples[step_key]),
                    },
                }

            test_entry["total_time_ms"] = _stats(test_total_samples)
            test_entry["total_time_per_repeat_ms"] = _stats(test_total_per_repeat_samples)

            run_summary["tests"].append(test_entry)
            run_summary["totals"][test_entry["status"]] = run_summary["totals"].get(test_entry["status"], 0) + 1

        for step_key, counters in step_counters.items():
            applicable = counters["applicable"]
            success_rate = (counters["success"] / applicable * 100) if applicable else 0.0
            run_summary["statistics"]["steps"][step_key] = {
                "counts": counters,
                "success_rate": success_rate,
                "time_total_ms": _stats(step_total_samples[step_key]),
                "time_per_repeat_ms": _stats(step_normalized_samples[step_key]),
            }

        run_summary["statistics"]["total_time_ms"] = _stats(total_times)
        run_summary["statistics"]["total_time_per_repeat_ms"] = _stats(total_per_repeat_times)
        return run_summary

    def run_test_suite(self, test_dir: str,
                  iterations: int = 3,
                  max_tests: Optional[int] = None) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –µ–¥–∏–Ω—ã–π run-summary."""
        print("\nüöÄ –ó–∞–ø—É—Å–∫ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤")
        print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {test_dir}")
        print(f"üîÑ –ò—Ç–µ—Ä–∞—Ü–∏–π –Ω–∞ —Ç–µ—Å—Ç: {iterations}")

        test_files = []
        for root, dirs, files in os.walk(test_dir):
            for file in files:
                if file.endswith('.json') and file != "statistics.json":
                    test_files.append(os.path.join(root, file))

        if max_tests and max_tests < len(test_files):
            test_files = test_files[:max_tests]

        total_tests = len(test_files)
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
        print("\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:")

        if total_tests == 0:
            print("‚ö†Ô∏è  –¢–µ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî –±—É–¥–µ—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –ø—É—Å—Ç–æ–π run_summary.")

        for i, test_file in enumerate(test_files, 1):
            test_name = os.path.splitext(os.path.basename(test_file))[0]
            self._render_inline_progress(f"üß™ [{i}/{total_tests}] {test_name} ...")
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    test_data = json.load(f)
                if "frame_of_discernment" not in test_data or "bba_sources" not in test_data:
                    raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ç–µ—Å—Ç–∞")
                sources_count = len(test_data.get("bba_sources", []))
                alphas = [0.1] * sources_count
                self.run_test(test_data=test_data, test_name=test_name, iterations=iterations, alphas=alphas)
                self._render_inline_progress(f"‚úÖ [{i}/{total_tests}] {test_name}")
                self._finish_inline_progress()
            except Exception as e:
                self._render_inline_progress(f"‚ùå [{i}/{total_tests}] {test_name}: {e}")
                self._finish_inline_progress()
                failed_test_result = {
                    "metadata": {
                        "test_name": test_name,
                        "adapter": self.adapter_name,
                        "timestamp": datetime.now().isoformat(),
                        "status": "failed_to_start"
                    },
                    "iterations": [],
                    "aggregated": {},
                    "error": str(e)
                }
                self._save_test_results(failed_test_result, test_name)
                self.results.append(failed_test_result)

        run_summary = self._create_run_summary(discovered_tests=len(test_files))
        self.artifact_manager.save_json("run_summary.json", run_summary, root_dir=True)
        self._create_final_text_report(run_summary)

        print("\n‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        return run_summary

    def _create_final_text_report(self, run_summary: Dict[str, Any]):
        """–°–æ–∑–¥–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –∏–∑ run_summary."""
        meta = run_summary.get("run_meta", {})
        totals = run_summary.get("totals", {})
        step_stats = run_summary.get("statistics", {}).get("steps", {})
        total_time = run_summary.get("statistics", {}).get("total_time_ms", {})
        total_time_per_repeat = run_summary.get("statistics", {}).get("total_time_per_repeat_ms", {})

        lines = [
            "=" * 90,
            "üìä RUN SUMMARY",
            f"Adapter: {meta.get('adapter', 'unknown')}",
            f"Run ID: {meta.get('run_id', 'unknown')}",
            f"Generated at: {meta.get('generated_at', '')}",
            f"Discovered tests: {meta.get('discovered_tests', 0)}",
            f"Executed tests: {meta.get('executed_tests', 0)}",
            "=" * 90,
            "",
            "Totals:",
            f"  ‚úÖ success: {totals.get('success', 0)}",
            f"  ‚ö†Ô∏è full_conflict: {totals.get('full_conflict', 0)}",
            f"  üö´ not_supported: {totals.get('not_supported', 0)}",
            f"  ‚ùå failed: {totals.get('failed', 0)}",
            "",
            "Step statistics:",
        ]

        for step in ("step1", "step2", "step3", "step4"):
            stat = step_stats.get(step, {})
            counts = stat.get("counts", {})
            time_total = stat.get("time_total_ms", {})
            time_per_repeat = stat.get("time_per_repeat_ms", {})
            lines.append(
                f"  {step}: applicable={counts.get('applicable', 0)}, success={counts.get('success', 0)}, "
                f"failed={counts.get('failed', 0)}, full_conflict={counts.get('full_conflict', 0)}, "
                f"not_supported={counts.get('not_supported', 0)}, success_rate={stat.get('success_rate', 0.0):.1f}%"
            )
            lines.append(
                f"      time_total_ms: mean={time_total.get('mean', 0.0):.2f}, min={time_total.get('min', 0.0):.2f}, "
                f"max={time_total.get('max', 0.0):.2f}, sample_count={time_total.get('sample_count', 0)}"
            )
            lines.append(
                f"      time_per_repeat_ms: mean={time_per_repeat.get('mean', 0.0):.2f}, min={time_per_repeat.get('min', 0.0):.2f}, "
                f"max={time_per_repeat.get('max', 0.0):.2f}, sample_count={time_per_repeat.get('sample_count', 0)}"
            )

        lines.extend([
            "",
            "Total test time (only fully successful samples):",
            f"  total_ms: mean={total_time.get('mean', 0.0):.2f}, min={total_time.get('min', 0.0):.2f}, "
            f"max={total_time.get('max', 0.0):.2f}, sample_count={total_time.get('sample_count', 0)}",
            f"  per_repeat_ms: mean={total_time_per_repeat.get('mean', 0.0):.2f}, min={total_time_per_repeat.get('min', 0.0):.2f}, "
            f"max={total_time_per_repeat.get('max', 0.0):.2f}, sample_count={total_time_per_repeat.get('sample_count', 0)}",
            "",
            "Errors:",
        ])

        errors = run_summary.get("statistics", {}).get("errors", {})
        if not errors:
            lines.append("  (none)")
        else:
            for error, tests in errors.items():
                lines.append(f"  - {error}")
                lines.append(f"    tests: {', '.join(sorted(set(tests)))}")

        self.artifact_manager.save_text("final_report.txt", "\n".join(lines), subdir="logs")

    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ —Ä–∞–Ω–Ω–µ—Ä–∞.
        –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –ø–æ–¥–∫–ª–∞—Å—Å–∞—Ö –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤.
        """
        # –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ—Ç
        # –ü–æ–¥–∫–ª–∞—Å—Å—ã –º–æ–≥—É—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤, —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏ —Ç.–¥.
        pass
